# HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 455678.0
python_gc_objects_collected_total{generation="1"} 70107.0
python_gc_objects_collected_total{generation="2"} 4041.0
# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC
# TYPE python_gc_objects_uncollectable_total counter
python_gc_objects_uncollectable_total{generation="0"} 0.0
python_gc_objects_uncollectable_total{generation="1"} 0.0
python_gc_objects_uncollectable_total{generation="2"} 0.0
# HELP python_gc_collections_total Number of times this generation was collected
# TYPE python_gc_collections_total counter
python_gc_collections_total{generation="0"} 51568.0
python_gc_collections_total{generation="1"} 4686.0
python_gc_collections_total{generation="2"} 128.0
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="11",patchlevel="7",version="3.11.7"} 1.0
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 1.06314989568e+011
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 6.354169856e+09
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.71592759687e+09
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 23900.43
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 96.0
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP vllm:cache_config_info information of cache_config
# TYPE vllm:cache_config_info gauge
vllm:cache_config_info{block_size="16",cache_dtype="auto",enable_prefix_caching="False",gpu_memory_utilization="0.9",num_cpu_blocks="2048",num_gpu_blocks="27798",num_gpu_blocks_override="None",sliding_window="None",swap_space_bytes="4294967296"} 1.0
# HELP vllm:num_requests_running Number of requests currently running on GPU.
# TYPE vllm:num_requests_running gauge
vllm:num_requests_running{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2.0
# HELP vllm:num_requests_waiting Number of requests waiting to be processed.
# TYPE vllm:num_requests_waiting gauge
vllm:num_requests_waiting{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1.0
# HELP vllm:num_requests_swapped Number of requests swapped to CPU.
# TYPE vllm:num_requests_swapped gauge
vllm:num_requests_swapped{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
# HELP vllm:gpu_cache_usage_perc GPU KV-cache usage. 1 means 100 percent usage.
# TYPE vllm:gpu_cache_usage_perc gauge
vllm:gpu_cache_usage_perc{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 4.2
# HELP vllm:cpu_cache_usage_perc CPU KV-cache usage. 1 means 100 percent usage.
# TYPE vllm:cpu_cache_usage_perc gauge
vllm:cpu_cache_usage_perc{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
# HELP vllm:prompt_tokens_total Number of prefill tokens processed.
# TYPE vllm:prompt_tokens_total counter
vllm:prompt_tokens_total{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1.06246478e+08
# HELP vllm:generation_tokens_total Number of generation tokens processed.
# TYPE vllm:generation_tokens_total counter
vllm:generation_tokens_total{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.12679e+06
# HELP vllm:time_to_first_token_seconds Histogram of time to first token in seconds.
# TYPE vllm:time_to_first_token_seconds histogram
vllm:time_to_first_token_seconds_bucket{le="0.001",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:time_to_first_token_seconds_bucket{le="0.005",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:time_to_first_token_seconds_bucket{le="0.01",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:time_to_first_token_seconds_bucket{le="0.02",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 57.0
vllm:time_to_first_token_seconds_bucket{le="0.04",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 706.0
vllm:time_to_first_token_seconds_bucket{le="0.06",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1240.0
vllm:time_to_first_token_seconds_bucket{le="0.08",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1802.0
vllm:time_to_first_token_seconds_bucket{le="0.1",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2380.0
vllm:time_to_first_token_seconds_bucket{le="0.25",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 6735.0
vllm:time_to_first_token_seconds_bucket{le="0.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 12222.0
vllm:time_to_first_token_seconds_bucket{le="0.75",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 18667.0
vllm:time_to_first_token_seconds_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 20771.0
vllm:time_to_first_token_seconds_bucket{le="2.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 32502.0
vllm:time_to_first_token_seconds_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 43021.0
vllm:time_to_first_token_seconds_bucket{le="7.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 48560.0
vllm:time_to_first_token_seconds_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 51656.0
vllm:time_to_first_token_seconds_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53694.0
vllm:time_to_first_token_seconds_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53694.0
vllm:time_to_first_token_seconds_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 149737.30035066605
# HELP vllm:time_per_output_token_seconds Histogram of time per output token in seconds.
# TYPE vllm:time_per_output_token_seconds histogram
vllm:time_per_output_token_seconds_bucket{le="0.01",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:time_per_output_token_seconds_bucket{le="0.025",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2.047565e+06
vllm:time_per_output_token_seconds_bucket{le="0.05",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2.933829e+06
vllm:time_per_output_token_seconds_bucket{le="0.075",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.004314e+06
vllm:time_per_output_token_seconds_bucket{le="0.1",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.030426e+06
vllm:time_per_output_token_seconds_bucket{le="0.15",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.032257e+06
vllm:time_per_output_token_seconds_bucket{le="0.2",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.033162e+06
vllm:time_per_output_token_seconds_bucket{le="0.3",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.035929e+06
vllm:time_per_output_token_seconds_bucket{le="0.4",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.038157e+06
vllm:time_per_output_token_seconds_bucket{le="0.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.039768e+06
vllm:time_per_output_token_seconds_bucket{le="0.75",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.042908e+06
vllm:time_per_output_token_seconds_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.045329e+06
vllm:time_per_output_token_seconds_bucket{le="2.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.054483e+06
vllm:time_per_output_token_seconds_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.073096e+06
vllm:time_per_output_token_seconds_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.073096e+06
vllm:time_per_output_token_seconds_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 202693.24174451828
# HELP vllm:e2e_request_latency_seconds Histogram of end to end request latency in seconds.
# TYPE vllm:e2e_request_latency_seconds histogram
vllm:e2e_request_latency_seconds_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 11340.0
vllm:e2e_request_latency_seconds_bucket{le="2.5",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 18422.0
vllm:e2e_request_latency_seconds_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 25286.0
vllm:e2e_request_latency_seconds_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 39101.0
vllm:e2e_request_latency_seconds_bucket{le="15.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 50636.0
vllm:e2e_request_latency_seconds_bucket{le="20.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 52415.0
vllm:e2e_request_latency_seconds_bucket{le="30.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53242.0
vllm:e2e_request_latency_seconds_bucket{le="40.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53564.0
vllm:e2e_request_latency_seconds_bucket{le="50.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:e2e_request_latency_seconds_bucket{le="60.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:e2e_request_latency_seconds_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:e2e_request_latency_seconds_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:e2e_request_latency_seconds_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 352430.06688022614
# HELP vllm:request_prompt_tokens Number of prefill tokens processed.
# TYPE vllm:request_prompt_tokens histogram
vllm:request_prompt_tokens_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:request_prompt_tokens_bucket{le="2.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:request_prompt_tokens_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 0.0
vllm:request_prompt_tokens_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 79.0
vllm:request_prompt_tokens_bucket{le="20.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 205.0
vllm:request_prompt_tokens_bucket{le="50.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 652.0
vllm:request_prompt_tokens_bucket{le="100.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1313.0
vllm:request_prompt_tokens_bucket{le="200.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2725.0
vllm:request_prompt_tokens_bucket{le="500.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 6847.0
vllm:request_prompt_tokens_bucket{le="1000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 13916.0
vllm:request_prompt_tokens_bucket{le="2000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 27333.0
vllm:request_prompt_tokens_bucket{le="5000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_prompt_tokens_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_prompt_tokens_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_prompt_tokens_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 1.06245675e+08
# HELP vllm:request_generation_tokens Number of generation tokens processed.
# TYPE vllm:request_generation_tokens histogram
vllm:request_generation_tokens_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 5484.0
vllm:request_generation_tokens_bucket{le="2.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 5484.0
vllm:request_generation_tokens_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 47452.0
vllm:request_generation_tokens_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 47452.0
vllm:request_generation_tokens_bucket{le="20.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 47453.0
vllm:request_generation_tokens_bucket{le="50.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 47453.0
vllm:request_generation_tokens_bucket{le="100.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 47454.0
vllm:request_generation_tokens_bucket{le="200.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 51335.0
vllm:request_generation_tokens_bucket{le="500.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 51335.0
vllm:request_generation_tokens_bucket{le="1000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 51335.0
vllm:request_generation_tokens_bucket{le="2000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_generation_tokens_bucket{le="5000.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_generation_tokens_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_generation_tokens_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_generation_tokens_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 3.12676e+06
# HELP vllm:request_params_best_of Histogram of the best_of request parameter.
# TYPE vllm:request_params_best_of histogram
vllm:request_params_best_of_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_bucket{le="2.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_bucket{le="20.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_best_of_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
# HELP vllm:request_params_n Histogram of the n request parameter.
# TYPE vllm:request_params_n histogram
vllm:request_params_n_bucket{le="1.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_bucket{le="2.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_bucket{le="5.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_bucket{le="10.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_bucket{le="20.0",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_bucket{le="+Inf",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_count{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
vllm:request_params_n_sum{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53693.0
# HELP vllm:request_success_total Count of successfully processed requests.
# TYPE vllm:request_success_total counter
vllm:request_success_total{finished_reason="length",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 53689.0
vllm:request_success_total{finished_reason="stop",model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 4.0
# HELP vllm:avg_prompt_throughput_toks_per_s Average prefill throughput in tokens/s.
# TYPE vllm:avg_prompt_throughput_toks_per_s gauge
vllm:avg_prompt_throughput_toks_per_s{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 2670.213349848995
# HELP vllm:avg_generation_throughput_toks_per_s Average generation throughput in tokens/s.
# TYPE vllm:avg_generation_throughput_toks_per_s gauge
vllm:avg_generation_throughput_toks_per_s{model_name="/home/data/models/Meta-Llama-3-8B-Instruct"} 154.6386182404132